# Избор на действие

Проверката за качество на софтуерни продукти често се извършва посредством автоматизирани, полуавтоматизирани или ръчно изпълняващи се тестове. Основна цел при създаване и изпълнение на тези тестове е постигане на високо или пълно покритие на създадения програмен код [@zhu1997software]. От своя страна, това покритие повишава възможността програмата да не достига непредвидени състояния и да притежава желаната функционалност [@ohba1982software].

Създаването на тестове, които проверяват цялостната функционалност на системата често се извършва от специалисти по проверка на качеството (QA). Те създават автоматизирани тестове или изпълняват проверката ръчно, спрямо предварително създадени спецификации. Част от тестовете, обхващащи целия софтуерен продукт се извършват спрямо графичния потребителски интерфейс (ГПИ), който този софтуер предоставя. Тези тестове (наречени ГПИ тестове (GUI tests)) симулират взаимодействието на потребител с програмата.

Създаването на автоматизирани ГПИ тестове е обвързано с трудности, като често променящи се визуални елементи, забавено изпълнение, достигане на непредвидени състояния на средата и др. [@memon2002gui]. Често, поради тази причини подобен вид тестове се изпълняват изцяло ръчно или полуавтоматизирано, което изисква взаимодействие с експерт.

QA експертът взаимодейства с ГПИ чрез поредица от действия (извършени чрез мишка, клавиатура, докосвания върху екран и/или др.), които променят ГПИ и водят до друго нейно състояние (в частност, нов екран). Когато това състояние не е наблюдавано до момента, покритието на програмен код се увеличава, поради нуждата от изпълнение му за създаване на самото състояние.

Тогава, целта при създаване на ГПИ тестове може да се определи като посещаване на всяко състояние на визуалната среда поне веднъж. Повторно наблюдение на дадено състояние може да е необходимо поради допълнителни възможни действия. Действията, които се избират, определят последователността на наблюдаваните състояния, както и бързодействието на текущия тест (минимален брой на взети действия за постигане на целта).

В тази част от работата ще създадем модел, който избира следващо действие, когато средата се намира в определено състояние. Това действие трябва да бъде избрано, така че да максимизира увеличението на покритие на програмен код и минимизира нуждата възможността за попадане във вече наблюдавано състояние. Състоянието на средата ще бъде закодирано чрез матрица, отговаряща на елементите в нея. Това е опростен подход към решаване на поставената задача, като той ще бъде разширен в следващата глава.

За решаване на задачата ще използваме подход управляван от наличните данни (data-driven approach). Конкретно, създаваме БИНМ (Бейсова Изкуствена Невронна Мрежа), която приема състоянието на средата като входен параметър и изчислява апостериорните разпределения на вероятностите за възможните действия за да оценим до колко добро е всяко от тях. Обучението на БИНМ  изисква предварително събрани данни.

## Литературен обзор

Съществуват различни подходи за автоматизирано създаване на ГПИ тестове за мобилни и уеб приложения: [@amalfitano2015mobiguitar], [@choi2013guided], [@moreira2014gui], [@salvesen2015using], [@moreira2017pattern] [@memon2002gui], но тяхната практическа употреба и ефективност са незадоволителни [@choudhary2015automated].

Предложеният подход е вдъхновен от работата представена в:

- [@mnih2015human] - използват се ИНМ за обучение на агент, който играе игри, надвишавайки възможностите на човек в някои от тези игри 
- [@pmlr-v70-shi17a] - среда, предоставяща възможност за създаване и обучение на агенти, изпълняващи задачи в уеб среди (напр. закупуване на самолетен билет)
- [@chang2010gui] - визуален скриптов език за създаване на тестове, който използва изображения за определяне на следващо действие

Текущият подход се различава по това, че:

- автоматизира напълно (или в голяма степен) създаването на ГПИ тестове
- предоставя среда, даващата информация за новото покритие на код при взимане на действие
- оценя несигурността (uncertainty) за избиране на действие, което може да е полезно за:
  - Състояния в които е необходима допълнителна информация за да бъде продължено изучаването на средата (напр. екран изискваш потребителско име и парола)   
  - достигнато е неочаквано състояние (аномалия), което може да е свързано с грешка в програмният код

### Обучение с учител (supervised learning)

Много практически проблеми могат да бъдат формулирани като намиране на функция $f: X \mapsto Y$, където $X$ е пространство на входните данни, а $Y$ е пространство на изходните данни. Често, дефинирането на $f$ е трудно или невъзможно. Например, каква е функцията, която намира позицията на бутон в изображение от ГПИ?

Обучението с учител предлага подход, който използва примерни данни имащи вида: $(x, y) \in X \times Y$, за да намери функция, която предоставя добри приближения на резултатите на $f$.

**Цел.** Нека имаме обучителна извадка $E$ от вероятностно разпределение $D$ съдържаща $n$ примера $\{(x_1, y_1), \ldots (x_n, y_n)\}$, които са независими и еднакво разпределени. *Обучение* наричаме търсенето на такава функция $f: X \mapsto Y$, която дава най-близки резултати до тези от обучителната извадка. Обучението се състои в избиране на функция на загубата/грешката (loss) $L(\hat{y}, y)$ измерваща несъгласието между предсказаната стойност $\hat{y} = f(x)$ и истинската стойност $y$. Целта на обучението е да намери $f^* \in \mathcal{F}$, която удовлетворява уравнението:

$$f^* = arg \min_{f \in \mathcal{F}}E_{(x, y) \sim D} L(f(x), y)$$ \label{eq:f}

където $\mathcal{F}$ е някакво множество от възможни функции. Обучението се свежда до търсене на такава $f^*$, която минимизира очаквана грешка над $D$.

Поставеният оптимизационен проблем е нерешим, защото нямаме достъп до всички елементи от $D$. Следователно, не може да намерим очаквана грешка или да я опростим, без да наложим силни предположения относно $D, L \, \text{или} \, f$. Ако използваме предположението за независимост и еднаква разпределеност може да получим приближение на очакваната грешка като използваме извадки от осреднената грешка върху обучителната извадка:

$$f^* \approx arg\min_{f \in \mathcal{F}} \frac{1}{n} \sum_{i=1}^n L(f(x_i), y_i)$$ \label{eq:f_approx}

Предполагаме, че минимизирането на грешката върху обучителната извадка $E$ ще ни даде таква функция $f$, която минимизира грешката върху всички стойности от $D$.

Нека разгледаме един от най-простите статистически модели - линейната регресия [@gauss1809theoria; @legendre1805nouvelles]. Нека е дадено множество от $n$ входно-изходни двойки $\{(x_1, y_1), ..., (x_n, y_n)\}$. Например, нека $x$ да е тегло в кг, а $y$ - височина в см на $n$ човека. Линейната регресия прави предположението, че съществува линейна функция, която преобразува всяко $x_i \in \mathbb{R}$ към $y_i \in \mathbb{R}$. Тогава нашият модел е линейна трансформация на входните данни:

$$y = f(x) = W^Tx + b$$

където $W$ и $b$ са параметри на модела, които трябва да оценим. Често използвана функция оценяща грешката е квадрата от разликата между предсказаната и истинската стойност:

$$L(\hat{y}, y) = (\hat{y} - y)^2$$

Тогава, задачата свеждаме до намиране на такива стойности за $W$ и $b$, които минимизират грешката:

$$f^* = arg\min_{w, b}[\frac{1}{n} \sum_{i=1}^n(W^Tx_i + b - y_i)^2]$$

Обучението на модел се свежда до оптимизационен проблем, който има следната обща форма $\theta^* = arg\min_{\theta} g(\theta)$, където $\theta$ са параметри на модела, а $g(\theta) = \frac{1}{n}\sum_{i=1}^n L(f_{\theta}(x_i), y_i)$.

### Оптимизация

Нека предположим, че $g$ е диференцируема. Тогава може да намерим градиента на $g$:

$$\nabla_{\theta}g = \theta \frac{\delta{g}}{\delta{\theta}} $$

Градиентът е вектор от частни производни даващ ни наклона на $g$ от всяко измерение за $\theta$. Той може да бъде използван за посока на търсене - може да подобрим оценката на $\theta$ като добавим малка стойност от негативната посока на градиента (тъй като търсим минимум на функция). Това мотивира създаването на алгоритъмът за постепенно спускане (gradient descent) [@cauchy1847methode], който работи на две стъпки:

1. намери градиента
2. обнови параметрите, като използваш малка стъпка в посока на негативния градиент

Процесът продължава докато не се достигне желаната грешка. Например, намирането на стойностите на $W$ се свежда до продължителното прилагане на:

$$W := W - \alpha\frac{\delta{g}}{\delta{W}}$$

Където $\alpha$ е размерът на стъпката. Ако стойността е прекалено висока, може да не се достигне сходимост, когато стойността на $\alpha$ е прекалено ниска обучението отнема прекалено дълго време.

### Изкуствени невронни мрежи (ИНМ)

В предишните раздели видяхме, че може да дефинираме произволна диференцируема функция, която съпоставя входни данни $x$ на предсказани стойности $\hat{y}$. За обучение на моделът използвахме постепенно спускане. Нека разгледаме по-подробно функцията $f$ и намиране на градиента, необходим за изпълнения на алгоритъма.

Построяването на изкуствена невронна мрежа се състой в повторение на умножение на матрици и прилагане на активационни функции. Активационните функции (напр. sigmoid, tanh, ReLU) позволяват на ИНМ да апроксимират нелинейни функции. 

Например, построяването на двуслойна невронна мрежа може да се представи като $f(x) = W_2 \sigma (W_1 x)$, където $W_1$ и $W_2$ са матрици, а $\sigma$ е активационна функция. Ако $\sigma$ е identity функция, ИНМ е линейна функция.

*Теорема* **Универсална теорема за апроксимация** изкуствена невронна мрежа с поне 1 скрит слой може да апроксимира произволна функция с произволна точност [@cybenko1989approximation], [@hornik1989multilayer].

**Биологично вдъхновение.** Изкуствените невронни мрежи са вдъхновени от груб модел на биологичния неврон. Всеки ред от матрицата с теглата $W$ моделира един неврон и силата на връзките от входните данни. Претеглената сума от входните данни пристига в тялото на клетката и се прилага активационна функция, което се интерпретира като скорост на предаване (firing rate) на неврона.

#### Backpropagation

### Бейсово моделиране

Ще представим математическия апарат в контекста на задачата, която ще решаваме.

Избирането на следващо действие по време на създаване на тестов случай пряко зависи от увереността във взимането на правилното решение. Несигурността от избиране на действие може да бъде моделирана посредством Бейсов подход.

Нека $\theta$ е неизвестна стойност, която може да е скаларна, вектор или матрица. Методите за статистически извод (inference) могат да ни помогнат да я намерим. Класическият статистически подход третира $\theta$ като фиксирана стойност. Единствената информация, която използваме за намиране на неизвестната стойност, идва от данните, с които разполагаме. Изводът се базира на резултат, получен от фунцкията на правдоподобие, която свързва стойности от $p(y|\theta)$ с всяка възможност на $\theta$, където $y = (y_1,...,y_n)$ е вектор с наблюдавани стойности.

Бейсовият подход третира $\theta$ като случайна величина. За достигане на извод се използва разпределението на параметри при дадени данни $p(\theta|y)$. Това разпределение се нарича апостериорно. Освен функцията на правдоподобие, Бейсовият подход включва априорно разпределение $p(\theta)$, което представя вярванията ни за $\theta$ преди да се разгледат данните.

Теоремата на Бейс дава връзка между фунцкията на правдоподобие и априорното разпределение:

$$p(\theta|y) = \frac{p(\theta|y)p(\theta)}{p(y)}$$

където:

$$p(y) = \int p(y|\theta)p(\theta)d\theta$$

Формулата на Бейс може да бъде пренаписана по следния начин:

\label{eq:bayes_simpler} $$p(\theta|y) \propto p(\theta|y)p(\theta)$$

тъй като $p(y)$ не зависи от $\theta$.

Когато $\theta$ е многомерна величина може да напишем уравнение \ref{eq:bayes_simpler} използвайки маргиналните апостериорни разпределения като например:

$$p(\theta_1|y) = \int p(\theta|y)d\theta_2$$

където $\theta = (\theta_1, \theta_2)$. В много случаи резултатите са многомерни и точни изводи не могат да бъдат направени, дори аналитично. Поради тази причина често използваме извадки от апостериорното разпределение.

**Пример.** Нека имаме обучителна извадка с размер $n$: $X = \{x_1, \ldots, x_n\}$ и $Y = \{y_1, \ldots, y_n\}$. Искаме да намерим параметрите $\theta$ на функцията $y = f^{\theta}(x)$, които са вероятно са използване за генериране на обучителната извадка.

Следвайки Бейсовият подход прилагаме априорно разпределение над параметрите, $p(\theta)$ и дефинираме функция на правдоподобие $p(y | x, \theta)$.

За класификационни задачи може да използваме softmax функция на правдоподобие:

$$p(y = d| x, \theta) = \frac{exp(f^{\theta}_d(x))}{\sum_{i=1}^n exp(f^{\theta}_i(x))}$$ \label{eq:softmax_likelihood}

или Гаусова функция на правдоподобие за регресионни задачи:

$$p(y | x, \theta) = \mathcal{N}(y;f^{\theta}(x), \tau^{-1}I)$$ \label{eq:gauss_likelihood}

където $\tau$ определя прецизността на модела.

При дадена обучителна извадка $(X, Y)$ може да намерим апостериорното разпределение за параметър $\theta$ използвайки правилото на Бейс:

$$p(\theta | X, Y) = \frac{p(Y | X, \theta)p(\theta)}{p(Y | X)}$$

Използвайки апостериорното разпределение може да направим изводи относно ненаблюдавани данни $x^*$:

$$p(y^*|x^*, X, Y) = \int p(y^*|x^*, \theta)p(\theta| X, Y)d\theta$$

**Намиране на апостериорно разпределение.** Основен компонент при намирането на апостериорното разпределение е знаменателят (нормализатор) в правилото на Бейс:

$$p(Y | X) = \int p(Y | X, \theta)p(\theta)d\theta$$

Това интегриране се нарича още маргинализация на функцията на правдоподобие над $\theta$. Маргинализацията може да бъде извършена аналитично за прости модели като Бейсовата линейна регресия. В подобни модели вероятностното разпределение на функцията на правдоподобие е спрегнато (conjugate) с това на вероятностното разпределение на функцията на апостериорното разпределение, което позволява аналитично решение. Такова решение не може да бъде намерено, когато моделите са по-сложни, защото искаме да приложим маргинализация върху всички възможни стойности на параметъра $\theta$. За оценяване на параметри в по-сложни модели може апроксимиращи методи.

### Бейсови изкуствени невронни мрежи (БИНМ)

Бейсовите изкуствени невронни мрежи, предложени в [@Tishby1989ConsistentIO] и подробно разгледани в [@mackay1992practical], [@mackay1992bayesian] и [@neal2012bayesian], предоставят вероятностна интерпретация чрез представяне теглата на изкуствените неврони като вероятностни разпределения. Тези модели са издръжливи на прекомерно нагаждане (overfitting), предоставят оценка на несигурността при взимане на решения и могат да се обучават с малки извадки.

БИНМ поставят априорно разпределение върху теглата на невронната мрежа. Най-често се използва Гаусово вероятностно разпределение, приложено върху матрицата на параметрите $p(W_i) = \mathcal{N}(0, 1)$. Функцията на правдоподобие се дефинира използвайки уравнение \ref{eq:softmax_likelihood} или \ref{eq:gauss_likelihood}.

#### MC Dropout

## Пример

Ще разгледаме мобилно приложение предоставящо възможност за поръчка на цветя с ГПИ представен на \ref{flower_store_navigation}. 

![*Примерен ГПИ на мобилно приложение предоставящо възможност за поръчка на цветя .*\label{flower_store_navigation}](./source/figures/flower_store_navigation.png)

ГПИ се състои от 4 различни състояния, като началното е маркирано със *Start*. 

Ще опростим задачата, като приложим "решетка", която разделя изображенията на средата на 4 правоъгълника с равни лица, визуализирани на \ref{flower_store_grid}.

![*Решетка приложена върху състотяние на средата*\label{flower_store_grid}](./source/figures/flower_store_grid.png)

Това представяне ни позволява да изпълняваме следните 5 различни действия:

- $a_1$ - клик горе в ляво
- $a_2$ - клик горе в дясно
- $a_3$ - клик долу в ляво
- $a_4$ - клик долу в дясно
- $a_5$ - връщане назад


Ще закодираме съдържанието на всяка клетка в решетката като:

- w - бял цвят, върху който не могат да бъдат предприемани действия (изображение или празно пространство)
- b - син цвят, който представя текстова информация
- g - зелен цвят, който представя бутон

Състояние *Start* може да закодираме като \ref{flower_store_encoding}


![*Закодиране на състояние Start*\label{flower_store_encoding}](./source/figures/flower_store_encoding.png)

Нека след първоначално обучение от специалист качество на софтуер (QA expert) имаме матрицата на преходите $T$, дефинирана като:

| $s_{x_1}$ | $s_{x_2}$ | $s_{x_3}$ | $s_{x_4}$ | action |
|-----------|-----------|-----------|-----------|--------|
| b         | w         | g         | g         | $a_3$  |
| b         | w         | w         | b         | $a_5$  |
| b         | w         | g         | g         | $a_4$  |

където  $s = (s_{x_1}, s_{x_2}, s_{x_3}, s_{x_4})$ е вектор от характеристиките на състоянието

Преходите от матрица $T$ може да се представят като:

![*Преходи на средата описани в $T$*\label{flower_store_transitions}](./source/figures/flower_store_transitions.png)

Получаваме ново състояние, което не е описано в $T$. Свеждаме задачата до пресмятане на апостериорните разпределения на действията и намиране на оптималното действие.

## Модел

Нека имаме среда $E$, намираща се в състояние $s \in \mathbb{S}$, върху което могат да бъдат изпълнени действия от множеството от действия $\mathbb{A}$. При избор на действие $a \in \mathbb{A}$, средата $E$  преминава в ново състояние $s'$ (в частност, $s' = s$, т.е. средата може да  не премине в ново състояние). Множеството $\mathbb{A}$ е ненаредено и всяко $a \in \mathbb{A}$ може да се обозначи с единствено цяло число, като по този начин въвеждаме наредба в $\mathbb{A}$. Всяко състояние на средата $S$ позволява изпълнението на действия $\mathbb{A}$, които са предварително дефинирани. Множеството от всички възможни състояния на средата $\mathbb{S}$ е неизвестно.

Нека след първоначално обучение от специалист имаме матрица на преходите $T$ с размерност $n \times 2$, където $n$ е броя на преходите. Всеки ред от $T$ дефинира наредена двойка $(състояние, действие)$, която описва оптималните действия за състоянията.

Нека имаме състояние $s'$, за което $T$ не съдържа информация. В този случай, целта е да намерим наредено подмножество от подходящи действия.

### Бейсова изкуствена невронна мрежа (БИНМ)

 Вероятностното разпределение над всички възможни действия би позволило оценяване на несигурността при избор на действие. С тази информация може да решим кога да използваме знанията за средата и кога да я изучаваме [@2018arXiv180204412A]. Когато вероятността е по-голяма ще е по-вероятно да изберем конкретното действие.

Ще използваме бейсова невронна мрежа със следната архитектура:

- входен слой: слой с 12 неврона
- първи скрит слой: пълно свързан слой (fully-connected) с 20 неврона
- втори скрит слой: пълно свързан слой (fully-connected) с 15 неврона
- изходен слой: слой с 5 неврона (броя на възможните действия)

Прилагаме ReLU активизационна функция, предложена в [@hahnloser2000digital] и отпадане (dropout), предложен в [@srivastava2014dropout], с вероятност за отпадане на неврон $p=0.2$  върху първи и втори скрит слой. Допълнително, прилагаме нормализация на група от данни (batch normalization), предложена в [@ioffe2015batch], след втори скрит слой. Описаната БИНМ може да се представи в PyTorch [@paszke2017pytorch] като:

```python
class Model(nn.Module):

    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(12, 20)
        self.drop1 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(20, 15)
        self.bn = nn.BatchNorm1d(15)
        self.drop2 = nn.Dropout(p=0.2)
        self.fc3 = nn.Linear(15, 5)

    def forward(self, x):
        x = F.relu(self.drop1(self.fc1(x)))
        x = F.relu(self.bn(self.drop2(self.fc2(x))))
        return self.fc3(x)
```


## Експерименти

В следващите експерименти ще приложим описания БИНМ модел върху 3 различни ГПИ среди. Към всяка среда имаме данни предоставени от QA експерт, закодирани по схемата представена в таблица на преходите $T$. 

Моделът ще оценим като използваме кръстосана ентропия (cross entropy) дефинирана като:

$$H(p, q) = E_p[-\log{q}] = H(p) + D_{KL}(p || q)$$

където p и q са вероятностни разпределения, съответно на предсказани и наблюдавани стойности, $H(p)$ е ентропията на $p$ и $D_{KL}(p || q)$ е Кулбак-Лейблер разстоянието до $q$ от $p$. За дискретни $p$ и $q$ имаме:

$$H(p, q) = - \sum_x p(x) \log q(x)$$

В проведените експерименти $H(p, q)$ има вида

$$H(p, q) = - \sum_{x=1}^5 p(x) \log q(x)$$

### Данни

Първата среда създадена за целите на текущата работа и е представена в описания пример по-горе.

Втората среда представя мобилното приложение *AutoMath Photo Calculator* и използва данни от Rico представени в [@deka2017rico]. Приложението дава решения на математически задачи след заснемането им с камерата на устройството. Част от визуалната среда е представена на \ref{auto_math_navigation}

![*Преходи при изучаване на AutoMath Photo Calculator* \label{auto_math_navigation}](./source/figures/auto_math_navigation.png)

Прилагайки предложения по-горе подход за закодиране върху тази среда получаваме \ref{auto_math_encoding}

![*Закодирано представяне на AutoMath Photo Calculator*\label{auto_math_encoding}](./source/figures/auto_math_encoding.png)

Третата среда е базирана на част от мобилното приложение *Memrise*. То предоставя флашкарти за изучаване на чужди езици и материали създадени от потребителя. 

![*Преходи при изучаване на Memrise* \label{memrise_navigation}](./source/figures/memrise_navigation.png)

Закодираната версия е представена на \ref{memrise_encoding}

![*Закодирано представяне на Memrise*\label{memrise_encoding}](./source/figures/memrise_encoding.png)

### Обучение

Целта е да минимизираме грешката $H(p, q)$. Ще използваме *SGD* оптимизатор със скорост на обучение (learning rate) $lr=0.1$ и движеща сила (momentum) $m=0.5$. Описаният модел обучаваме за *1,000* епохи върху всяка среда по отделно. Записваме грешката върху тренировъчните данни след обучението на модела във всяка епоха.

### Резултати

За извличане на извадки от апостериорното разпределение на действията използваме алгоритъм Монте Карло отпадане (MC dropout) и прилагаме отпадане по време на тестване $n=10,000$ пъти. Стойностите във всяка извадка се нормализират - отместваме всички стойности с минималната стойност от извадката и сумата от всички стойности приравняваме на единица.

#### Примерна среда

![*Промяна на грешката по време на обучение върху примерната среда*\label{training_loss}](./source/figures/training_loss.png)

От графиката се вижда как след първите 50 епохи от обучението, средната грешка намалява до стойност близка до 0.5 и след това остава постоянна. В идеалния случай грешката трябва да намалява постепенно и да достигне стойности близки до 0. Ако графиката много бързо достига до 0, моделът се влияе прекалено силно от обучителната извадка.

Матрицата $T$ предоставя малко данни за примерната среда (само 3 реда), но въпреки това предложеният модел успява да намали тренировъчната грешка. Разбира се, възможно е модела да се е нагодил (overfit) спрямо данните, които използваме за обучение. Средната стойност и стандартното отклонение на апостериорното разпределение на вероятността действието да е оптимално са дадени на \ref{action_dist_flower}.

|      | $a_1$ | $a_2$ | $a_3$ | $a_4$ | $a_5$ |
|------|-------|-------|-------|-------|-------|
| mean | 0.006 | 0.025 | 0.427 | 0.434 | 0.107 |
| std  | 0.010 | 0.018 | 0.036 | 0.031 | 0.073 |

\label{action_dist_flower}

От таблица \ref{action_dist_flower} виждаме, че действие $a_4$ е оптималното действие за последното наблюдавано състояние.

Последното състояние получено от примерната среда при изпълнение на действията описани в $T$ е представено на \ref{flower_store_final_state}

![Закодиране на последното състояние представено в примерната среда\label{flower_store_final_state}](./source/figures/flower_store_final_state.png)

При неколкократно изпълнение за обучение и оценка на модела наблюдаваме, че получените извадки се различават и понякога $a_3$ има по-висока средна стойност от $a_4$. Причината за това е малкото количество данни в $T$.

Апостериорното разпределение е представено на \ref{posterior_actions_dist} и обобщено в \ref{posterior_actions_box}

![*Апостериорно разпределение за всяко действие*\label{posterior_actions_dist}](./source/figures/posterior_actions_dist.png)

![*Апостериорно разпределение за всяко действие*\label{posterior_actions_box}](./source/figures/posterior_actions_box.png)

#### AutoMath

Резултатите от прилагането на предложеният модел за втората среда, използвайки реални данни от Rico, са представени в следващите графики:

![*Промяна на грешката по време на обучение върху AutoMath*\label{auto_math_training_loss}](./source/figures/auto_math_training_loss.png)

![*Апостериорно разпределение за всяко действие*\label{auto_math_posterior_dist}](./source/figures/auto_math_posterior_dist.png)



![*Апостериорно разпределение за всяко действие*\label{auto_math_posterior_box}](./source/figures/auto_math_posterior_box.png)

От графика \ref{auto_math_trainig_loss} се вижда как след първите 500 епохи от обучението, средната грешка клони към 0. От графиките \ref{auto_math_posterior_dist} и \ref{auto_math_posterior_box} се вижда, че моделът е много сигурен в избора на действие $a_4$ като оптимално.

#### Memrise

![*Промяна на грешката по време на обучение върху Memrise*\label{memrise_training_loss}](./source/figures/memrise_training_loss.png)

![*Апостериорно разпределение за всяко действие*\label{memrise_posterior_dist}](./source/figures/memrise_posterior_dist.png)

![*Апостериорно разпределение за всяко действие*\label{memrise_posterior_box}](./source/figures/memrise_posterior_box.png)

От графика \ref{memrise_trainig_loss} се вижда как след първите 400 епохи от обучението, средната грешка клони към 0. От графиките \ref{memrise_posterior_dist} и \ref{memrise_posterior_box} се вижда, че моделът е много сигурен в избора на действие $a_4$ като оптимално и в отхвърлянето на действие $a_5$.

#### Използване на всички данни

Промяната на $H(p, q)$ по време на обучение използвайки всички налични данни е представена на \ref{combined_training_loss}. 

![*Промяна на грешката по време на обучение върху всички данни*\label{combined_training_loss}](./source/figures/combined_training_loss.png)

Получихме желаното поведение на грешката.

![*Апостериорно разпределение за всяко действие*\label{combined_posterior_dist}](./source/figures/combined_posterior_dist.png)

![*Апостериорно разпределение за всяко действие*\label{combined_posterior_box}](./source/figures/combined_posterior_box.png)

Графики \ref{combined_posterior_dist} и \ref{combined_posterior_box} представят предсказани стойности от модела, когато използваме състояние \ref{} от примерната среда. Наблюдаваме, че вече моделът е много сигурен в избора на действие $a_4$

## Заключение

Разработихме модел, който може да избира оптимално следващо действие при представено закодирано състояние на ГПИ среда. Допълнително, моделът ни дава възможност да оценим до колко сигурен е в предсказаните стойности. Моделът се състои от дълбока Бейсова невронна мрежа, която получава закодирано състояние на средата като входни данни и е обучена използвайки кръстосана ентропия оценяща грешката. Извадки от апостериорното разпределение на действията извлекохме използвайки MC dropout. Оценката на обучените модели показва добра ефективност при избор на действие и генерализация между различните среди, когато използваме данни от всички среди.

Едно от ограниченията на предложения модел е предположението, че имаме външен агент, който закодира изображенията на средата до представяне, което е подходящо за обучение. В глава 5 ще разширим предложения модел, така че входните данни да са "сурови" - изображенията, които средата предоставя. Друго ограничение на нашия модел е, че не взима под предвид последователността от взетите действия по време на изучаването на ГПИ средата. Следователно, агентът има възможност да опита да изучи състояния от средата, които вече е изучил.
