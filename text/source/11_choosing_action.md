# Избор на действие

Проверката за качество на софтуерни продукти често се извършва посредством автоматизирани, полуавтоматизирани или ръчно изпълняващи се тестове. Основна цел при създаване и изпълнение на тези тестове е постигане на високо или пълно покритие на създадения програмен код [@zhu1997software]. От своя страна, това покритие повишава възможността програмата да не достига непредвидени състояния и да притежава желаната функционалност [@ohba1982software].

Създаването на тестове, които проверяват цялостната функционалност на системата често се извършва от специалисти по проверка на качеството (QA). Те създават автоматизирани тестове или изпълняват проверката ръчно, спрямо предварително създадени спецификации. Част от тестовете, обхващащи целия софтуерен продукт се извършват спрямо графичния потребителски интерфейс (ГПИ), който този софтуер предоставя. Тези тестове (наречени ГПИ тестове (GUI tests)) симулират взаимодействието на потребител с програмата.

Създаването на автоматизирани ГПИ тестове е обвързано с трудности, като често променящи се визуални елементи, забавено изпълнение, достигане на непредвидени състояния на средата и др. [@memon2002gui]. Често, поради тази причини подобен вид тестове се изпълняват изцяло ръчно или полуавтоматизирано, което изисква взаимодействие с експерт.

QA експертът взаимодейства с ГПИ чрез поредица от действия (извършени чрез мишка, клавиатура, докосвания върху екран и/или др.), които променят ГПИ и водят до друго нейно състояние (в частност, нов екран). Когато това състояние не е наблюдавано до момента, покритието на програмен код се увеличава, поради нуждата от изпълнение му за създаване на самото състояние.

Тогава, целта при създаване на ГПИ тестове може да се определи като посещаване на всяко състояние на визуалната среда поне веднъж. Повторно наблюдение на дадено състояние може да е необходимо поради допълнителни възможни действия. Действията, които се избират, определят последователността на наблюдаваните състояния, както и бързодействието на текущия тест (минимален брой на взети действия за постигане на целта).

В тази част от работата ще създадем модел, който избира следващо действие, когато средата се намира в определено състояние. Това действие трябва да бъде избрано, така че да максимизира увеличението на покритие на програмен код и минимизира нуждата възможността за попадане във вече наблюдавано състояние. Състоянието на средата ще бъде закодирано чрез матрица, отговаряща на елементите в нея. Това е опростен подход към решаване на поставената задача, като той ще бъде разширен в следващата глава.

За решаване на задачата ще използваме подход управляван от наличните данни (data-driven approach). Конкретно, създаваме БИНМ (Бейсова Изкуствена Невронна Мрежа), която приема състоянието на средата като входен параметър и изчислява апостериорните разпределения на вероятностите за възможните действия за да оценим до колко добро е всяко от тях. Обучението на БИНМ  изисква предварително събрани данни.

## Литературен обзор

Съществуват различни подходи за автоматизирано създаване на ГПИ тестове за мобилни и уеб приложения: [@amalfitano2015mobiguitar], [@choi2013guided], [@moreira2014gui], [@salvesen2015using], [@moreira2017pattern] [@memon2002gui], но тяхната практическа употреба и ефективност са незадоволителни [@choudhary2015automated].

Предложеният подход е вдъхновен от работата представена в:

- [@mnih2015human] - използват се ИНМ за обучение на агент, който играе игри, надвишавайки възможностите на човек в някои от тези игри 
- [@pmlr-v70-shi17a] - среда, предоставяща възможност за създаване и обучение на агенти, изпълняващи задачи в уеб среди (напр. закупуване на самолетен билет)
- [@chang2010gui] - визуален скриптов език за създаване на тестове, който използва изображения за определяне на следващо действие

Текущият подход се различава по това, че:

- автоматизира напълно (или в голяма степен) създаването на ГПИ тестове
- предоставя среда, даващата информация за новото покритие на код при взимане на действие
- оценя несигурността (uncertainty) за избиране на действие, което може да е полезно за:
  - Състояния в които е необходима допълнителна информация за да бъде продължено изучаването на средата (напр. екран изискваш потребителско име и парола)   
  - достигнато е неочаквано състояние (аномалия), което може да е свързано с грешка в програмният код

## Пример

Ще разгледаме мобилно приложение предоставящо възможност за поръчка на цветя с ГПИ представен на \ref{flower_store_navigation}. 

![*Примерен ГПИ на мобилно приложение предоставящо възможност за поръчка на цветя .*\label{flower_store_navigation}](./source/figures/flower_store_navigation.png)

ГПИ се състои от 4 различни състояния, като началното е маркирано със *Start*. 

Ще опростим задачата, като приложим "решетка", която разделя изображенията на средата на 4 правоъгълника с равни лица, визуализирани на \ref{flower_store_grid}.

![*Решетка приложена върху състотяние на средата*\label{flower_store_grid}](./source/figures/flower_store_grid.png)

Това представяне ни позволява да изпълняваме следните 5 различни действия:

- $a_1$ - клик горе в ляво
- $a_2$ - клик горе в дясно
- $a_3$ - клик долу в ляво
- $a_4$ - клик долу в дясно
- $a_5$ - връщане назад


Ще закодираме съдържанието на всяка клетка в решетката като:

- w - бял цвят, върху който не могат да бъдат предприемани действия (изображение или празно пространство)
- b - син цвят, който представя текстова информация
- g - зелен цвят, който представя бутон

Състояние *Start* може да закодираме като \ref{flower_store_encoding}


![*Закодиране на състояние Start*\label{flower_store_encoding}](./source/figures/flower_store_encoding.png)

Нека след първоначално обучение от специалист качество на софтуер (QA expert) имаме матрицата на преходите $T$, дефинирана като:

| $s_{x_1}$ | $s_{x_2}$ | $s_{x_3}$ | $s_{x_4}$ | action |
|-----------|-----------|-----------|-----------|--------|
| b         | w         | g         | g         | $a_3$  |
| b         | w         | w         | b         | $a_5$  |
| b         | w         | g         | g         | $a_4$  |

където  $s = (s_{x_1}, s_{x_2}, s_{x_3}, s_{x_4})$ е вектор от характеристиките на състоянието

Преходите от матрица $T$ може да се представят като:

![*Преходи на средата описани в $T$*\label{flower_store_transitions}](./source/figures/flower_store_transitions.png)

Получаваме ново състояние, което не е описано в $T$. Свеждаме задачата до пресмятане на апостериорните разпределения на действията и намиране на оптималното действие.


## Модел

Нека имаме среда $E$, намираща се в състояние $s \in \mathbb{S}$, върху което могат да бъдат изпълнени действия от множеството от действия $\mathbb{A}$. При избор на действие $a \in \mathbb{A}$, средата $E$  преминава в ново състояние $s'$ (в частност, $s' = s$, т.е. средата може да  не премине в ново състояние). Множеството $\mathbb{A}$ е ненаредено и всяко $a \in \mathbb{A}$ може да се обозначи с единствено цяло число, като по този начин въвеждаме наредба в $\mathbb{A}$. Всяко състояние на средата $S$ позволява изпълнението на действия $\mathbb{A}$, които са предварително дефинирани. Множеството от всички възможни състояния на средата $\mathbb{S}$ е неизвестно.

Нека след първоначално обучение от специалист имаме матрица на преходите $T$ с размерност $n \times 2$, където $n$ е броя на преходите. Всеки ред от $T$ дефинира наредена двойка $(състояние, действие)$, която описва оптималните действия за състоянията.

Нека имаме състояние $s'$, за което $T$ не съдържа информация. В този случай, целта е да намерим наредено подмножество от подходящи действия.

### Бейсова изкуствена невронна мрежа (БИНМ)

 Вероятностното разпределение над всички възможни действия би позволило оценяване на несигурността при избор на действие. С тази информация може да решим кога да използваме знанията за средата и кога да я изучаваме [@2018arXiv180204412A]. Когато вероятността е по-голяма ще е по-вероятно да изберем конкретното действие.

Ще използваме бейсова невронна мрежа със следната архитектура:

- входен слой: слой с 12 неврона
- първи скрит слой: пълно свързан слой (fully-connected) с 20 неврона
- втори скрит слой: пълно свързан слой (fully-connected) с 15 неврона
- изходен слой: слой с 5 неврона (броя на възможните действия)

Прилагаме ReLU активизационна функция, предложена в [@hahnloser2000digital] и отпадане (dropout), предложен в [@srivastava2014dropout], с вероятност за отпадане на неврон $p=0.2$  върху първи и втори скрит слой. Допълнително, прилагаме нормализация на група от данни (batch normalization), предложена в [@ioffe2015batch], след втори скрит слой. Описаната БИНМ може да се представи в PyTorch [@paszke2017pytorch] като:

```python
class Model(nn.Module):

    def __init__(self):
        super(Model, self).__init__()
        self.fc1 = nn.Linear(12, 20)
        self.drop1 = nn.Dropout(p=0.2)
        self.fc2 = nn.Linear(20, 15)
        self.bn = nn.BatchNorm1d(15)
        self.drop2 = nn.Dropout(p=0.2)
        self.fc3 = nn.Linear(15, 5)

    def forward(self, x):
        x = F.relu(self.drop1(self.fc1(x)))
        x = F.relu(self.bn(self.drop2(self.fc2(x))))
        return self.fc3(x)
```


## Експерименти

В следващите експерименти ще приложим описания БИНМ модел върху 3 различни ГПИ среди. Към всяка среда имаме данни предоставени от QA експерт, закодирани по схемата представена в таблица на преходите $T$. Моделът ще оценим като използваме кръстосана ентропия (cross entropy) дефинирана като:

$$H(p, q) = E_p[-\log{q}] = H(p) + D_{KL}(p || q)$$

където $H(p)$ е ентропията на $p$ и $D_{KL}(p || q)$ е Кулбак-Лейблер разстоянието до $q$ от $p$. За дискретни $p$ и $q$ имаме:

$$H(p, q) = - \sum_x p(x) \log q(x)$$

### Данни

Първата среда създадена за целите на текущата работа и е представена в описания пример по-горе.

Втората среда представя мобилното приложение *AutoMath Photo Calculator* и използва данни от Rico представени в [@deka2017rico]. Приложението дава решения на математически задачи след заснемането им с камерата на устройството. Част от визуалната среда е представена на \ref{auto_math_navigation}

![*Преходи при изучаване на AutoMath Photo Calculator* \label{auto_math_navigation}](./source/figures/auto_math_navigation.png)

Използвайки подхода за закодиране приложен върху тази среда получаваме \ref{auto_math_encoding}

![*Закодирано представяне на AutoMath Photo Calculator*\label{auto_math_encoding}](./source/figures/auto_math_encoding.png)

Третата среда е базирана на част от мобилното приложение *Memrise*. То предоставя флашкарти за изучаване на чужди езици и материали създадени от потребителя. 

![*Преходи при изучаване на Memrise* \label{memrise_navigation}](./source/figures/memrise_navigation.png)

Закодираната версия е представена на \ref{memrise_encoding}

![*Закодирано представяне на Memrise*\label{memrise_encoding}](./source/figures/memrise_encoding.png)

### Обучение

За оптимизатор използваме *SGD* със скорост на обучение (learning rate) $lr=0.1$ и движеща сила (momentum) $m=0.5$. Описаният модел обучаваме за *1,000* епохи върху всяка среда по отделно. Записваме грешката върху тренировъчните данни след обучението на модела във всяка епоха.

### Резултати

За извличане на извадки от апостериорното разпределение на действията използваме Монте Карло отпадане (MC dropout) и прилагаме отпадане по време на тестване $n=10,000$ пъти. Стойностите във всяка извадка се нормализират - отместваме всички стойности с минималната стойност от извадката и сумата от всички стойности приравняваме на единица.

#### Примерна среда

![*Промяна на грешката по време на обучение върху примерната среда*\label{training_loss}](./source/figures/training_loss.png)

Матрицата $T$ предоставя малко данни за примерната среда (само 3 реда), но въпреки това предложения модел успява да намали тренировъчната грешка. Разбира се, възможно е модела да се е нагодил (overfit) спрямо данните, които използваме за обучение. Средната стойност и стандартното отклонение на апостериорното разпределение за вероятността действие да е оптимално са дадени на \ref{action_dist_flower}.

|      | $a_1$ | $a_2$ | $a_3$ | $a_4$ | $a_5$ |
|------|-------|-------|-------|-------|-------|
| mean | 0.006 | 0.025 | 0.427 | 0.434 | 0.107 |
| std  | 0.010 | 0.018 | 0.036 | 0.031 | 0.073 |

\label{action_dist_flower}

Отново, закодираното последно състояние получено от примерната среда при изпълнение на действията описани в $T$ е представено на \ref{flower_store_final_state}

![Закодиране на последното състояние представено в примерната среда\label{flower_store_final_state}](./source/figures/flower_store_final_state.png)

При неколкократно изпълнение за обучение и оценка на модела наблюдаваме, че получените извадки се различават и понякога $a_3$ има по-висока средна стойност от $a_4$.

Апостериорното разпределение е представено на \ref{posterior_actions_dist} и \ref{posterior_actions_box}

![*Апостериорно разпределение за всяко действие*\label{posterior_actions_dist}](./source/figures/posterior_actions_dist.png)

![*Апостериорно разпределение за всяко действие*\label{posterior_actions_box}](./source/figures/posterior_actions_box.png)

## Заключение
