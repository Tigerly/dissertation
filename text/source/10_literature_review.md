# Съвременни методи за машинно самообучение

## Подсилено обучение

Една от основните задачи в сферата на изкуствения интелект е взимане на поредица от решения в стохастична среда. Един конкретен пример за взимане на решения в стохастична среда е агент, който изучава ГПИ. Тази задача се състои в избор на редица от решения, които да максимизират броят на разгледаните състояния на текущото приложение. Това е по-сложно от задачи, в които трябва да се направи само едно решение. Оценката за представянето на агента може да се даде само след много извършени от него стъпки. Това означава, че той може да избере неправилно действие сега и да разбере за това много по-късно, т.е. имаме *забавяне на последствията*. Допълнително, не може да наблюдаваме точното състояние на средата, поради липсата на точен модел на средата, която се изучава.

Основният начин за моделиране на такива среди са Марковски вериги.

**Марковските вериги за вземане на решения (MDP)** моделират системи, които искаме да контролираме. Във всяка времева стъпка $t$, системата се намира в дадено състояние $s$. Например, описаният агент може да се намира на даден екран от приложението, след като е натиснал определен бутон. Системата преминава през различни състояния като резултат от действията, които сме избрали. Задачата ни е да избираме действия, които са добри и да минимизираме броя на тези, които не са. Разнообразни проблеми са моделирани чрез Марковски вериги (MDP формализма). Някои примери за използване на марковски вериги са системи за препоръки [@joachims1997webwatcher], рутиране на мрежи [@boyan1994packet], управление на асансьори [@crites1996improving], навигация на роботи [@sutton1998reinforcement].

Подсиленото обучение (RL) [@sutton1998reinforcement] дава начини за решаване на задачи, дефинирани чрез MDP формализма. Самообучаващ се агент с подсилено обучение (RL) взаимодейства със средата за определено време. На всяка времева стъпка $t$, агентът получава състояние $s_t$ от пространството на състоянията $S$ и избира действие $a_t$ от пространство с действия $A$, следвайки политика $\pi(a_t|s_t)$. Политиката $\pi$ определя поведението на агента, т.е. в определено състояние $s_t$, какво действие агентът трябва да избере. Тя дава функция за преобразуване на състояние $s_t$ до състояние $s_{t + 1}$ чрез действие $a_t$. Използвайки дадена политика, агентът получава скаларна награда $r_t$ и преминава в следващо състояние $s_{t + 1}$, което се определя от функцията за награди $R(s, a)$ и функцията, даваща вероятности за преминаване в друго състояние $P(s_{t+1}|s_t,a_t)$. Когато моделът, който моделира поведението на агента е дискретен, т.е. може да се разглежда като отделни епизоди, описаният процес продължава докато агентът не достигне до крайно състояние. Тогава агентът се рестартира за започване на ново обучение. Общата награда е дефинирана като:

$$R_t = \sum_{k=0}^{\infty}\gamma^kr_{t+k}$$

представлява обезценена стойност с фактор $\gamma \in (0,1]$. Агентът се опитва да максимизира очакваната награда във всяко състояние.

Функция на стойностите $Q^\pi(s, a)$ дава предсказана обща бъдеща награда, която измерва до колко са добри дадено състояние или двойка състояние-действие. Стойността на дадено действие $Q^\pi(s, a) = E[R_t|s_t = s, a_t = a]$ ни дава очакваната награда за избиране на дейсвие $a$ в състояние $s$ и следвайки фиксирана политика $\pi$. Оптимална стойностна функция $Q^*(s,a)$ предоставя действие $a$, което максимизира стойността на наградата за дадено състояние $s$. Може да дефинираме функция даваща стойност на състоянията $V^\pi(s)$, както и оптималната й версия $V^*(s)$ по сходен начин.

Извод: ние подсилваме обучението с въвеждане на награда.

### Дълбоко обучение

Нека разгледаме един от най-простите статистически модели - линейната регресия [@gauss1809theoria; @legendre1805nouvelles]. Нека е дадено множество от $N$ входно-изходни двойки $\{(x_1, y_1), ..., (x_n, y_n)\}$. Например, нека $x$ да е тегло в кг, а $y$ - височина в см на $N$ човека. Линейната регресия прави предположението, че съществува линейна функция, която преобразува всяко $x_i \in \mathbb{R}^Q$ към $y_i \in \mathbb{R}^D$. Тогава нашият модел е линейна трансформация на входните данни:

$$f(x) = xW + b$$

където $W$ е $Q \times D$ матрица и $b$ е вектор от $D$ елемента. Тогава, задачата се свежда до намиране на такива параметри $W$ и $b$, които минимизират средната квадратична грешка:

$$e = \frac{1}{N}\sum_i||y_i - (x_iW + b)||^2$$

В общия случай, връзката между $x$ и $y$ може да не е линейна. Тогава искаме да дефинираме нелинейна функция $f(x)$, която преобразува входните данни до изходни. За тази цел може да приложим linear basis function regression (превод?) [@bishop2007pattern; @gergonne1815application], където входните данни $x$ се подават на $K$ фиксирани скаларни нелинейни трансформации $\phi_k(x)$ за създаване на свойствен вектор $\Phi(x) = [\phi_1(x), ...,\phi_k(x)]$. Трансформациите $\phi_k$ наричаме базисни функции. Върху така създадения вектор се прилага линейна регресия. LBFR може да се сведе до линейна регресия, когато $\phi_k(x) := x_k$ и $K = Q$. Този тип функции се смятат за фиксирани и взаимно ортогонални. Когато тези ограничения се пропуснат говорим за *параметризирани* базисни функции.

#### Изкуствени невронни мрежи

Когато подредим параметризирани базисни функции в йерархия, може да говорим за изкуствени невронни мрежи. Всеки свойствен вектор в тази йерархия ще наричаме слой. Композицията от подобни слоеве води до голямата гъвкавост на тези модели. Често те постигат високи резултати на различни задачи и могат да се приложат върху реални проблеми, работещи върху терабайти от данни.

**Feed-forward neural networks.** Нека разгледаме модел с един *скрит слой* [@rumelhart1985learning]. Нека $x$ е вектор с $Q$ елемента, представящ входните данни. Трансформираме го с афинна трансформация до вектор с $K$ елемента. Отбелязваме с $W_1$ линейната преобразуваща матрица (матрица на теглата) и с $b$ транслацията използвана за трансформиране на $x$ за да получим $xW_1 + b$. Върху всеки елемент на получената матрица се прилага нелинейна функция $\sigma(\cdot)$. Резултатът е т. нар. *скрит слой*, а всеки елемент се нарича *мрежова единица*. Върху резултата се прилага втора линейна трансформация с матрица на теглата $W_2$, която преобразува скрития слой до изходен вектор с $D$ елемента. Имаме $Q \times K$ матрица $W_1$, $K \times D$ матрица $W_2$ и $b$ - вектор от $K$ елемента. Резултат от дадена невронна мрежа би бил:

$$\hat{y} = \sigma(xW_1 + b)W_2$$

при дадени входни данни $x$.

Когато използваме невронната мрежа за решаване на регресионна задача, може да минимизираме Евклидовата грешка:

$$ e^{W_1, W_2, b}(X, Y) = \frac{1}{2N}\sum_{i=1}^{N}||y_i - \hat{y_i}||^2$$

където $\{y_1,\dots,y_n\}$ са $N$ наблюдавани изходни стойности, $\{\hat{y_1},\dots,\hat{y_n}\}$ са изходни данни от модела, а $\{x_1,\dots,x_n\}$ са входните данни. Предполагаме, че минимизирайки тази грешка спрямо $W_1, W_2, b$ ще получим модел, който генерализира добре при нови данни $X_{\text{test}}, Y_{\text{test}}$.

Когато задачата е да се предскаже класът, към който $x$ принадлежи, от множеството $\{1,\dots,D\}$, използваме същия модел. Промяната се състой в това, че прилагаме softmax функция върху получения резултат. Тази функция ни дава нормализирани оценки за всеки клас:

$$\hat{p_i} = \frac{exp(\hat{y_i})}{\sum d' exp(\hat{y_i'})}$$

Когато вземем логаритъма от горната функция, получаваме softmax грешка:

$$ e^{W_1, W_2, b}(X, Y) = -\frac{1}{N}\sum_{i=1}^{N}log(\hat{p}_{i, c_i})$$

където $c_i \in \{1, \dots, D\}$ е наблюдаваният клас за вход $i$.

Описаният по-горе модел има проста структура, но може да бъде разширен за по-специализирани задачи. Този тип по-сложни модели се използват, когато задачите изискват обработка на поредици или изображения.

**Convolutional Neural Networks** CNN е архитектура [@lecun1989backpropagation], която се използва при изображения. Задачи, които до скоро се смятаха за нерешими, имат решения посредством този тип модели [@hinton2012improving]. Моделът е създаден чрез рекурсивно приложение на конволуции и обединяващи слоеве. Конволуционният слой е линейна трансформация, която запазва пространствена информация от входното изображение.

**Recurrent neural networks (RNN)** RNN е модел [@rumelhart1985learning; @werbos1988generalization], базиран на поредици от данни, който се използва за обработка на текст, обработка на видео и други [@kalchbrenner2013recurrent; @sundermeyer2012lstm]. Входните данни за RNN са поредица от символи. За всяка времева стъпка $t$, проста невронна мрежа е приложена върху единствен символ, както и изходните данни от мрежата от предишната стъпка.

Конкретно, при дадена редица от входни данни $x = [x_1,\dots,x_t]$ с дължина $T$, прост RNN модел е създаден чрез повтарящо се приложение на функция $f_h$. Така се генерира скрито състояние $h_t$ за времева стъпка $t$:

$$h_t = f_h(x_t,h_{t-1}) = \sigma(x_tW_h + h_{t-1}U_h + b_h)$$

за някаква нелинейна функция $\sigma$. Изходните данни от модела може да бъдат дефенирани като:

$$\hat{y} = f_y(h_T) = h_TW_y + b_y$$

Съществуват и по-сложни RNN модели, като LSTM [@hochreiter1997long] и GRU [@cho2014learning]. 

### Дълбоко подсилено обучение

Този тип методи се класифицират, когато използваме дълбоки невронни мрежи за апроксимиране на някой от компонентите на подсиленото обучение: функция на стойностите $V(s;\theta)$, политика $\pi(a|s;\theta)$ или модела за промяна на състояние и награди. Параметрите $\theta$ представляват тегла в дълбоки невронни мрежи. Когато използваме "плитки" модели, като например линейна регресия, дървета за вземане на решения и др. като апроксиматори на функция, имаме "плитко" подсилено обучение с параметри $\theta$ за съответния модел. Основната разлика между дълбокото и плиткото подсилено обучение се състой в апроксиматора на функцията, която използват. Когато се използва извън политикова апроксимация - например на нелинейни функции, може да се наблюдават нестабилност и разходимост [@tsitsiklis1997analysis]. Въпреки това, скорошната работа върху дълбоки $Q$-мрежи [@mnih2015human] и *AlphaGo* [@silver2016alphago] стабилизират процеса на обучение и постигат много добри резултати.

Дълбокото подсилено обучение започна рязкото си развитие с работата на [@mnih2015human]. Преди това, RL даваше нестабилни резултати, когато се използваха нелинейни апроксиматори като невронни мрежи. Дълбоките $Q$ мрежи (DQN) направиха няколко важни приноса: 1) стабилизиране на обучението, използвайки дълбоки невронни мрежи [@lin1992self] 2) подход за цялостно обучение без почти никакво познание за областта 3) обучаване на гъвкава невронна мрежа с еднакъв алгоритъм за изпълняване на различни задачи, например 49 Atari игри [@bellemare2013arcade], на които се представят по-добре от всеки известен алгоритъм до момента.

#### Double DQN

[@van2016deep] предложиха Double DQN (D-DQN) за справяне с проблема на прекалена увереност (overestimate?) на Q-learning алгоритъма. В базовият алгоритъм (както и в DQN), параметрите се обновяват според:

$$\theta_{t + 1} = \theta_t + \alpha(y_t^{\theta} - Q(s_t, a_t; \theta_t))\Delta_{\theta_t}Q(s_t,a_t;\theta_{t})$$

където

$$y_t^Q = r_{t + 1} + \gamma\max\limits_{\alpha}Q(s_{t+1},a;\theta_t)$$

така че оператора $\max$ използва еднакви стойности, за да избере и оцени дадено действие. Като следствие от това, е по-вероятно да избере недостатъчно добри стойности. Double DQN предлага да оцени алчната политика спрямо невронна мрежа, но използва друга, за да оцени стойността й. Това може да се постигне с малка промяна на DQN алгоритъма, заменяме $y_t^Q$ с:

$$y_t^{D - DQN} = r_{t +1} + \gamma Q(s_{t+1},\max\limits_{\alpha}Q(s_{t+1},a_t;\theta_t);\theta_{\bar{t}})$$

където $\theta_t$ е параметър за първата невронна мрежа, а $\theta_{\bar{t}}$ е параметър за целевата мрежа.

#### Асинхронни методи

[@mnih2016asynchronous] предложи асинхронни методи за четири RL алгоритъма: Q-learning, SARSA, $n$-step Q-learning and advantage actor-critic и asynchronous advantage actor-critic (A3C). Този подход използва паралелни агенти, които използват различни политики за изучаване на средата. Асинхронните методи могат да се изпълняват върху многоядрени процесори. Те се изпълняват много по-бързо и предоставят по-бързо обучение от други известни методи.

