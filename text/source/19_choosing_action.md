# Избор на действие

## Литературен обзор

## Модел

Нека имаме среда $E$, намираща се в състояние $s \in \mathbb{S}$, върху което могат да бъдат изпълнени действия от множеството от действия $\mathbb{A}$. При избор на действие $a \in \mathbb{A}$, средата $E$ предоставя награда $r$, която приема стойности в интервала $[0; 1]$ и преминава в ново състояние $s'$ (в частност, $s' = s$, т.е. средата може да  не премине в ново състояние). Множеството $\mathbb{A}$ е ненаредено и всяко $a \in \mathbb{A}$ може да се обозначи с единствено цяло число, като по този начин въвеждаме наредба в $\mathbb{A}$. Всяко състояние на средата $S$ позволява изпълнението на действия $\mathbb{A}$, които са предварително дефинирани. Множеството от всички възможни състояния на средата $\mathbb{S}$ е неизвестно.

Нека след първоначално обучение от специалист имаме матрица на преходите $D$ с размерност $n \times 3$, където $n$ е броя на преходите. Всеки ред от $D$ дефинира наредена тройка $(състояние, действие, награда)$, която описва получените награди при изпълнение на съответното действие за даденото състояние.

Нека имаме състояние $s'$, за което $D$ не съдържа информация. В този случай, целта е да намерим подмножеството от действията, така че изпълнението им да води до получаване на оптимална награда от средата $E$.

Задачата е решена, когато $D$ съдържа информация за всички състояния $s \in \mathbb{S}$.

## Пример

Експертът е обучил ... като се стреми да посети всяко състояние поне по веднъж и минимизира избраните действия.

Нека имаме визуална среда с 5 възможни действия:

- $a_1$ - клик горе в ляво
- $a_2$ - клик горе в дясно
- $a_3$ - клик долу в ляво
- $a_4$ - клик долу в дясно
- $a_5$ - връщане назад

Нека след първоначално обучение от специалист качество на софтуер (QA expert) имаме матрицата на преходите D, дефинирана като:

| $s_{x_1}$ | $s_{x_2}$ | $s_{x_3}$ | $s_{x_4}$ | action | reward |
|-----------|-----------|-----------|-----------|--------|--------|
| b         | w         | g         | g         | $a_3$  | 0.25   |
| b         | w         | w         | b         | $a_5$  | 0.00   |
| b         | w         | g         | g         | $a_4$  | 0.25   |

където:

- $s = (s_{x_1}, s_{x_2}, s_{x_3}, s_{x_4})$ е вектор от характеристиките на състоянието
- w - бял цвят, върху който не могат да бъдат предприемани действия
- b - син цвят, който представя текстова информация
- g - зелен цвят, който представя бутон

Първото състояние (първият ред от матрицата $D$) може да се визуализира като:

![](./assets/ui_env_state_1.png)

Избрано е действие $a_3$ за което е получена награда $0.25$. Средата преминава в ново състояние (втори ред от $D$):

![](./assets/ui_env_state_2.png)

Тук е избрано действие $a_5$ - връщане назад и получаваме награда 0. Това ни връща в първоначалното състояние:

![](./assets/ui_env_state_1.png)

Тук сме избрали $a_4$ и сме получили награда от $0.25$. Получаваме ново състояние:

![](./assets/ui_env_state_3.png)

Получихме ново състояние, което не е описано в $D$. Свеждаме задачата до избор на действие, което ще ни даде максимална награда за текущото състояние.

## Обучение на модела

Задачата се свежда до намиране на действие, което предоставя най-висока награда за текущото състояние. Допълнително, вероятностното разпределение над всички възможни действия би позволило оценяване на несигурността при избор на действие. С тази информация може да решим кога да използваме знанията за средата и кога да я изучаваме []. Когато вероятността е по-голяма ще е по-вероятно да изберем конкретното действие.

### Бейсова невронна мрежа (БНМ)

## Експерименти

## Заключение
