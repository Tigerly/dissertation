# Литературен обзор

## Подсилено обучение

Един от основните проблеми в сферата на изкуствения интелект е взимане на поредица от решения в стохастична система. Агент, който изучава приложение, е пример за такава среда. Този проблем се състои в избора на редица от решения, които да максимизират разгледаните състояния на текущото приложение. Това е по-сложно от задачи, в които трябва да се направи само едно решение. Оценката за представянето на агента може да се даде само след много извършени стъпки. Това означава, че той може да избере неправилно действие сега и да разбере за това много по-късно, т.е. имаме *забавяне на последствията*. Допълнително, не може да наблюдаваме точното състояние на агента, поради липсата на точен модел на приложението, което тества.

**Марковски вериги за вземане на решения (MDP)** Моделират системи, които искаме да контролираме. Във всяка времева стъпка $t$, системата се намира в дадено състояние $s$. Например, описаният агент може да се намира на даден екран от приложението, след като е натиснал определен бутон. Системата преминава през различни състояния като резултат от действията, които сме избрали. Задачата ни е да избираме действия, които са добри и да минимизираме броя на тези, които не са. Разнообразни проблеми са моделирани чрез MDP формализма. Някои примери са системи за препоръки [@joachims1997webwatcher], рутиране на мрежи [@boyan1994packet], управление на асансьори [@crites1996improving], навигация на роботи [@sutton1998reinforcement].

Подсиленото обучение (RL) [@sutton1998reinforcement] дава способи за решаване на проблеми, дефинирани чрез MDP формализма. RL агент взаимодейства със среда за определено време. На всяка времева стъпка $t$, агентът получава състояние $s_t$ и избира действие $a_t$ от пространство с действия $A$, следвайки политика $\pi(a_t|s_t)$. Политиката $\pi$ определя поведението на агента. Тя дава функция за преобразуване на състояние $s_t$ до действия $a_t$. Използвайки дадена политика, агентът получава скаларна награда $r_t$ и преминава в следващо състояние $s_{t + 1}$, което се определя от функцията за награди $R(s, a)$ и функцията, даваща вероятности за преминаване в друго състояние $P(s_{t+1}|s_t,a_t)$. Когато проблемът е дискретен, т.е. може да се разглежда като отделни епизоди, описаният процес продължава докато агентът не достигне до крайно състояние и се рестартира. Общата награда, дефинирана като:

$$R_t = \sum_{k=0}^{\infty}\gamma^kr_{t+k}$$

представлява обезценена стойност с фактор $\gamma \in (0,1]$. Агентът се опитва да максимизира очакваната стойност за такава дългосрочна награда във всяко състояние.

Функция на стойностите дава предсказана обща бъдеща награда, която измерва до колко са добри дадено състояние или двойка състояние-действие. Стойността на дадено действие $Q^\pi(s, a) = E[R_t|s_t = s, a_t = a]$ ни дава очакваната награда за избиране на дейсвие $a$ в състояние $s$ и следвайки политика $\pi$. Оптимална стойностна функция $Q^*(s,a)$ предоставя действие $a$, което максимизира стойността на наградата за дадено състояние $s$. Може да дефинираме функция даваща стойност на състоянията $V^\pi(s)$, както и оптималната й версия $V^*(s)$ по сходен начин.

### Дълбоко обучение

Нека разгледаме един от най-простите статистически модели - линейната регресия [@gauss1809theoria; @legendre1805nouvelles]. Нека е дадено множество от $N$ входно-изходни двойки $\{(x_1, y_1), ..., (x_n, y_n)\}$. Например, нека $x$ да е тегло в кг, а $y$ - височина в см на $N$ човека. Линейната регресия прави предположението, че съществува линейна функция, която преобразува всяко $x_i \in \mathbb{R}^Q$ към $y_i \in \mathbb{R}^D$. Тогава нашият модел е линейна трансформация на входните данни:

$$f(x) = xW + b$$

където $W$ е $Q \times D$ матрица и $b$ е вектор от $D$ елемента. Тогава, задачата се свежда до намиране на такива параметри $W$ и $b$, които минимизират средната квадратична грешка:

$$e = \frac{1}{N}\sum_i||y_i - (x_iW + b)||^2$$

В общия случай, връзката между $x$ и $y$ може да не е линейна. Тогава искаме да дефинираме нелинейна функция $f(x)$, която преобразува входните данни до изходни. За тази цел може да приложим linear basis function regression (превод?) [@bishop2007pattern; @gergonne1815application], където входните данни $x$ се подават на $K$ фиксирани скаларни нелинейни трансформации $\phi_k(x)$ за създаване на свойствен вектор $\Phi(x) = [\phi_1(x), ...,\phi_k(x)]$. Трансформациите $\phi_k$ наричаме базисни функции. Върху така създадения вектор се прилага линейна регресия. LBFR може да се сведе до линейна регресия, когато $\phi_k(x) := x_k$ и $K = Q$. Този тип функции се смятат за фиксирани и взаимно ортогонални. Когато тези ограничения се пропуснат говорим за *параметризирани* базисни функции.

#### Изкуствени невронни мрежи

Когато подредим параметризирани базисни функции в йерархия, може да говорим за изкуствени невронни мрежи. Всеки свойствен вектор в тази йерархия ще наричаме слой. Композицията от подобни слоеве води до голямата гъвкавост на тези модели. Често те постигат високи резултати на различни задачи и могат да се приложат върху реални проблеми, работещи върху терабайти от данни.

**Feed-forward neural networks.** Нека разгледаме модел с един *скрит слой* [@rumelhart1985learning]. Нека $x$ е вектор с $Q$ елемента, представящ входните данни. Трансформираме го с афинна трансформация до вектор с $K$ елемента. Отбелязваме с $W_1$ линейната преобразуваща матрица (матрица на теглата) и с $b$ транслацията използвана за трансформиране на $x$ за да получим $xW_1 + b$. Върху всеки елемент на получената матрица се прилага нелинейна функция $\sigma(\cdot)$. Резултатът е т. нар. *скрит слой*, а всеки елемент се нарича *мрежова единица*. Върху резултата се прилага втора линейна трансформация с матрица на теглата $W_2$, която преобразува скрития слой до изходен вектор с $D$ елемента. Имаме $Q \times K$ матрица $W_1$, $K \times D$ матрица $W_2$ и $b$ - вектор от $K$ елемента. Резултат от дадена невронна мрежа би бил:

$$\hat{y} = \sigma(xW_1 + b)W_2$$

при дадени входни данни $x$.

Когато използваме невронната мрежа за решаване на регресионна задача, може да минимизираме Евклидовата грешка:

$$ e^{W_1, W_2, b}(X, Y) = \frac{1}{2N}\sum_{i=1}^{N}||y_i - \hat{y_i}||^2$$

където $\{y_1,\dots,y_n\}$ са $N$ наблюдавани изходни стойности, $\{\hat{y_1},\dots,\hat{y_n}\}$ са изходни данни от модела, а $\{x_1,\dots,x_n\}$ са входните данни. Предполагаме, че минимизирайки тази грешка спрямо $W_1, W_2, b$ ще получим модел, който генерализира добре при нови данни $X_{\text{test}}, Y_{\text{test}}$.

Когато задачата е да се предскаже класът, към който $x$ принадлежи, от множеството $\{1,\dots,D\}$, използваме същия модел. Промяната се състой в това, че прилагаме softmax функция върху получения резултат. Тази функция ни дава нормализирани оценки за всеки клас:

$$\hat{p_i} = \frac{exp(\hat{y_i})}{\sum d' exp(\hat{y_i'})}$$

Когато вземем логаритъма от горната функция, получаваме softmax грешка:

$$ e^{W_1, W_2, b}(X, Y) = -\frac{1}{N}\sum_{i=1}^{N}log(\hat{p}_{i, c_i})$$

където $c_i \in \{1, \dots, D\}$ е наблюдаваният клас за вход $i$.

Описаният по-горе модел има проста структура, но може да бъде разширен за по-специализирани задачи. Този тип по-сложни модели се използват, когато задачите изискват обработка на поредици или изображения.

**Convolutional Neural Networks** CNN е архитектура [@lecun1989backpropagation], която се използва при изображения. Задачи, които до скоро се смятаха за нерешими, имат решения посредством този тип модели [@hinton2012improving]. Моделът е създаден чрез рекурсивно приложение на конволуции и обединяващи слоеве. Конволуционният слой е линейна трансформация, която запазва пространствена информация от входното изображение.

**Recurrent neural networks (RNN)** RNN е модел [@rumelhart1985learning; @werbos1988generalization], базиран на поредици от данни, който се използва за обработка на текст, обработка на видео и други [@kalchbrenner2013recurrent; @sundermeyer2012lstm]. Входните данни за RNN са поредица от символи. За всяка времева стъпка $t$, проста невронна мрежа е приложена върху единствен символ, както и изходните данни от мрежата от предишната стъпка.

Конкретно, при дадена редица от входни данни $x = [x_1,\dots,x_t]$ с дължина $T$, прост RNN модел е създаден чрез повтарящо се приложение на функция $f_h$. Така се генерира скрито състояние $h_t$ за времева стъпка $t$:

$$h_t = f_h(x_t,h_{t-1}) = \sigma(x_tW_h + h_{t-1}U_h + b_h)$$

за някаква нелинейна функция $\sigma$. Изходните данни от модела може да бъдат дефенирани като:

$$\hat{y} = f_y(h_T) = h_TW_y + b_y$$

Съществуват и по-сложни RNN модели, като LSTM [@hochreiter1997long] и GRU [@cho2014learning]. 

### Дълбоко подсилено обучение

Този тип методи се класифицират, когато използваме дълбоки невронни мрежи за апроксимиране на някой от компонентите на подсиленото обучение: функция на стойностите $V(s;\theta)$, политика $\pi(a|s;\theta)$ или модела за промяна на състояние и награди. Параметрите $\theta$ представляват тегла в дълбоки невронни мрежи. Когато използваме "плитки" модели, като например линейна регресия, дървета за вземане на решения и др. като апроксиматори на функция, имаме "плитко" подсилено обучение с параметри $\theta$ за съответния модел. Основната разлика между дълбокото и плиткото подсилено обучение се състой в апроксиматора на функцията, която използват. Когато се използва извън политикова апроксимация - например на нелинейни функции, може да се наблюдават нестабилност и разходимост [@tsitsiklis1997analysis]. Въпреки това, скорошната работа върху дълбоки $Q$-мрежи [@mnih2015human] и *AlphaGo* [@silver2016alphago] стабилизират процеса на обучение и постигат много добри резултати.

Дълбокото подсилено обучение започна рязкото си развитие с работата на [@mnih2015human]. Преди това, RL даваше нестабилни резултати, когато се използваха нелинейни апроксиматори като невронни мрежи. Дълбоките $Q$ мрежи (DQN) направиха няколко важни приноса: 1) стабилизиране на обучението, използвайки дълбоки невронни мрежи [@lin1992self] 2) подход за цялостно обучение без почти никакво познание за областта 3) обучаване на гъвкава невронна мрежа с еднакъв алгоритъм за изпълняване на различни задачи, например 49 Atari игри [@bellemare2013arcade], на които се представят по-добре от всеки известен алгоритъм до момента.

#### Double DQN

[@van2016deep] предложиха Double DQN (D-DQN) за справяне с проблема на прекалена увереност (overestimate?) на Q-learning алгоритъма. В базовият алгоритъм (както и в DQN), параметрите се обновяват според:

$$\theta_{t + 1} = \theta_t + \alpha(y_t^{\theta} - Q(s_t, a_t; \theta_t))\Delta_{\theta_t}Q(s_t,a_t;\theta_{t})$$

където

$$y_t^Q = r_{t + 1} + \gamma\max\limits_{\alpha}Q(s_{t+1},a;\theta_t)$$

така че оператора $\max$ използва еднакви стойности, за да избере и оцени дадено действие. Като следствие от това, е по-вероятно да избере недостатъчно добри стойности. Double DQN предлага да оцени алчната политика спрямо невронна мрежа, но използва друга, за да оцени стойността й. Това може да се постигне с малка промяна на DQN алгоритъма, заменяме $y_t^Q$ с:

$$y_t^{D - DQN} = r_{t +1} + \gamma Q(s_{t+1},\max\limits_{\alpha}Q(s_{t+1},a_t;\theta_t);\theta_{\bar{t}})$$

където $\theta_t$ е параметър за първата невронна мрежа, а $\theta_{\bar{t}}$ е параметър за целевата мрежа.

#### Асинхронни методи

[@mnih2016asynchronous] предложи асинхронни методи за четири RL алгоритъма: Q-learning, SARSA, $n$-step Q-learning and advantage actor-critic и asynchronous advantage actor-critic (A3C). Този подход използва паралелни агенти, които използват различни политики за изучаване на средата. Асинхронните методи могат да се изпълняват върху многоядрени процесори. Те се изпълняват много по-бързо и предоставят по-бързо обучение от други известни методи.

## Бейсова статистика

Избиране на следващо действие по време на създаване на тестов случай пряко зависи от увереността във взимането на правилното решение. Несигурността от избиране на действие може да бъде моделирана посредством Бейсов подход.

Нека $\theta$ е неизвестна стойност, която може да е скаларна, векторна или матрица. Методите за статистически извод (inference) могат да ни помогнат да я намерим. Класическият статистически подход третира $\theta$ като фиксирана стойност. Единствената информация, която използваме за намиране на неизвестната стойност, идва от данните, с които разполагаме. Изводът се базира на резултат, получен от фунцкията на правдоподобие на $\theta$, която свързва стойности от $p(y|\theta)$ с всяка възможност на $\theta$, където $y = (y_1,...,y_n)$ е вектор с наблюдавани стойности.

Бейсовият подход третира $\theta$ като случайна стойност. За достигане на извод се използва разпределението на параметри при дадени данни $p(\theta|y)$. Това разпределение се нарича апостериорно. Освен функцията на правдоподобие, Бейсовият подход включва априорно разпределение $p(\theta)$, което представя вярванията ни за $\theta$ преди да се разгледат данните.

Теоремата на Бейс дава връзка между фунцкията на правдоподобие и априорното разпределение:

$$p(\theta|y) = \frac{p(\theta|y)p(\theta)}{p(y)}$$

където:

$$p(y) = \int p(y|\theta)p(\theta)d\theta$$

Формулата на Бейс може да бъде пренаписана по следния начин:

(@ref_for_eqn1) $$p(\theta|y) \propto p(\theta|y)p(\theta)$$

тъй като $p(y)$ не зависи от $\theta$

Когато $\theta$ е многомерна величина може да напишем уравнение (@ref_for_eqn1) използвайки маргиналните апостериорни разпределения като например:

$$p(\theta_1|y) = \int p(\theta|y)d\theta_2$$

където $\theta = (\theta_1, \theta_2)$. В много случаи резултатите са многомерни и точни изводи може да бъдат направени само аналитично. Поради тази причина често се използват приближения.

### Монте Карло алгоритми за Марковски Вериги (MCMC)

MCMC алгоритмите правят неявно интегриране като взимат извадки от апостериорното разпределение. По този начин се намират приближения на стойностите, от които се интересуваме.

В съществото си тези методи създават Марковска верига с апостериорното разпределение на параметрите като стационарно разпределение. Когато веригата е крайна и повтаряща се, стойността на $\theta$ може да бъде оценена от извадки на средни пътища. Генерираните извадки $\theta^{(t)}, t=1, \ldots, N$ от това разпределение дават представа за целевото разпределение.

#### Метрополис-Хастингс алгоритъм

Този алгоритъм е предложен от Metropolis [@metropolis1953equation] и по-късно генерализиран от Hastings [@hastings1970monte]. Методът създава Марковска верига с желаното стационарно разпределение. Алгоритъмът избира кандидат стойност $\theta'$ от предварително избрано разпределение $q(\theta, \theta')$, където $\theta' \neq \theta$. Избраната стойност $\theta'$ се проверява чрез приеми-откажи метод (accept-reject step), за да се подсигури, че принадлежи на целевото разпределение.

#### Извадки на Гибс

Този метод, предложен от Geman и Geman [@geman1984stochastic], често се представя като специален случай на Метрополис-Хастингс алгоритъма.

### Извод със свободни вариационни параметри

Variational Inference (VI) методите обикновено предлагат по-добри резултати спрямо MCMC, когато времето за изпълнение е ограничено. Допълнително предимство на тези подходи е, че те са детерминирани. Систематичната грешка и дисперсията се приближават до 0 при MCMC методите, за колкото повече време бъдат оставени да се изпълняват те. Тези свойства правят MCMC алгоритмите много ефективни на теория. В практиката обаче, времето за изпълнение и изчислителната мощ са ограничени. Това налага търсенето на по-бързо методи дори когато това намаля точността на получените резултати.

Този тип методи дефинират приближено вариационно разпределение $q_\omega(\theta)$, параметризирано от $\omega$, с лесна за оценяване структура. Искаме приближеното разпределение да е максимално близко до това на апостериорното. За целта свеждаме задачата до оптимизационна и минимизираме Kullback-Leibler (KL) [@kullback1951information] отклонението спрямо $\omega$. Интуитивно, KL измерва приликата между две разпределения:

$$KL(q\omega(\theta)\,||\,p(\theta|x, y)) = \int q\omega(\theta)log\frac{q\omega(\theta)}{p(\omega|x, y)}d\omega$$

(Define x,y - dataset)

Този интеграл е дефиниран, когато $q\omega(\theta)$ е непрекъсната спрямо $p(\theta|x, y)$. Нека $q^*_\omega(\theta)$ е минимизираща точка (може да е локален минимум). Тогава KL може да ни даде приближение на апостериорното разпределение:

$$p(y^*|x^*, x, y) \approx \int p(y^*|x^*, \theta)q^*_\omega(\theta)d\theta =: q^*_\omega(y^*|x^*)$$

VI методите заменят изчисляването на интеграли с такова на производни. Това е много подобно на оптимизационните методи използвани в DL. Основната разлика се състой в това, че оптимизацията е върху разпределения, а не точкови оценки. Този подход запазва много от предимствата на Бейсовото моделиране и представя вероятностни модели, които дават оценка на несигурността в изводите си.

### Бейсови Невронни Мрежи

Един от големите недостатъци на съществуващите архитектури на невронни мрежи е, че изводите, които получаваме, са оценки на точки. Моделите не казват до колко са сигурни в предложените резултати. Когато например един лекар получи резултат от даден модел, той трябва да знае защо и как моделът е стигнал до него. Бейсовата статистика може да даде отговор на тези въпроси [@gal2015dropout]. Дори при модели използващи RNN, Бейсова интерпретация на задачата дава по-добри резултати от съществуващи такива [@gal2016theoretically].

Бейсови невронни мрежи, предложени в края на 80-те години [@kononenko1989bayesian] и задълбочено изучавани по-късно [@mackay1992practical; @neal2012bayesian], предлагат вероятностна интерпретация на моделите за дълбоко обучение, като представят теглата им като вероятностни разпределения. Този тип модели са устойчиви на пренастройване (overfitting), предлагат оценки на несигурността и могат да се тренират върху малко на брой данни.

## Автоматизирано тестване на ГПИ

Проверката за правилно поведение на софтуер продукт е неизменна част от създаването му. Откриване и поправяне на всички потенциални проблеми преди той да бъде доставен до крайния потребител може да се сметне за най-добър случай.

### Автоматизирано тестване на Android приложения

Мобилните приложения също имат нужда от проверка на качеството. Поради тази причина, в последните години засилено се разглеждат начини за автоматизацията на подобен вид тестове. Много голяма част от извършената работа до момента се състои в създаване на входни данни за приложения за мобилната операционна система Android. Подходите използвани до момента, се различават по начина, по който създават входнни данни и изучават и използват евристики за приложението.

#### Съществуващи системи

**Dynodroid** [@machiry2013dynodroid] е инструмент, който се базира на случайно изучаване. Предлага се и ръчен начин за въвеждане на входнни данни, когато системата е заседнала.

**MobiGUITAR** [@amalfitano2015mobiguitar] строи модел на приложението по време на тестване. За всяко ново състояние се поддържа списък с възможни действия, които се изпълняват използвайки DFS (depth first search) стратегия.

**SwiftHand** [@choi2013guided] се опитва да максимизира покритието на код за тестваното приложение. Допълнително, инструментът се старае да минимизира броя рестартирания на приложението. SwiftHand генерира единствено докосвания и скролвания.

**PUMA** [@hao2014puma] предлага генерална среда за автоматизиране на ГПИ. Инструментът предлага рамка за програмиране, в която могат да бъдат имплементирани различни стратегии за изучаване на тестваното приложение.

#### Покритие на код

BBoxTester: [@zhauniarovich2015towards] 
CovDroid: [@yeh2015covdroid] 
ABCA: [@huang2015abca]

#### Текущо състояние (State of the art?)

[@choudhary2015automated]

### Проверка на качеството

- Достатъчно бързо ли е? (Model should monitor for speed exec anomalies or report just slow parts)
- Как да повторя грешката? (Provide/execute steps for reproduction)
- Има ли разлики в изходните данни? (Change in hierarchy/image screenshot)
