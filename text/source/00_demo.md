# Обобщение

Целта на настоящата работа е да създаде математически модел на самообучаващ се агент и да реализира агент, който напълно изследва дадена среда на Графичен Потребителски Интерфейс (ГПИ). Като входни данни за модела се използват изображения от ГПИ и информация за неговата сегментация. Агентът може да извършва действия, променяйки състоянието на ГПИ средата. Адекватността на извършените действия се оценява с награда, която средата предоставя. Наградата се определя от различни фактори, един от които е процентът покрит нов програмен код. Агентът успешно е изследвал средата, когато покритието на код на средата е пълно.

На базата на този модел ще създадем агент, който ще намира грешки в програмни продукти, ще синтезира пакети от автоматични тестове за качеството на софтуера и ще изпълнява посочени от потребителя задачи.

## Цели на дисертацията

Целта на дисертацията е да дефинира политика $\pi$, определяща поведението на агента.

- Дефиниране на множество от възможни действия на агента (пространтство на действията)
- Създаване на модел, който избира действията на агента
- Създаване на среда (за тестване и използване на агента) в която ще работи агента
  - Подбиране на приложения (applications)
  - Измерване на покритието на код
  - Създаване на изображение и сегментация от текущото състояние на средата
- Избор на метрики за оценка на действията на агента
- Предварително обучение (с учител) на агента с данни от хора, взаимодействащи със средата (imitation learning)
- Провеждане на експерименти и анализ на постигнатите резултати

## Модел на агента

### Пространство на състоянието (State space)

Дадено състояние $s$ на средата съдържа цветно изображение $I$ и информация за сегментацията DOM модел $D$, т.е. $s = (I, D)$. Изображението има размер $W \times H \times 3$, където $W$ е широчината на изображението в пиксели, $H$ височината на изображението в пиксели и 3 - броя на цветовете в палитрата (червено, зелено и синьо (rgb)). DOM моделът е представен като списък от текстови елементи, а информацията за сегментацията на изображението е дадена от наредената четворка $(x, y, w, h)$ $x$ абцисната координата, $y$ ординатната ос, $w$ - широчината на сегмента, $h$ - височина на сегмента.

### Пространство на действията (Action space)

Позицията на курсора $m = (m_x, m_y) \in [0, W) \times [0, H)$ се моделира чрез мултиномиално разпределение върху възможните позиции. ГПИ средата не изисква наличие на клавиатура, защото . Възможните действия са: click, drag, scroll-up, scroll-down.

### Модел за избор на действия

Агентът избира действие $a$ във време $t$, когато се намира в състояние $s$. Решението на агента се взима благодарение на Дълбока Бейсова Невронна Мрежа (Deep Baysian Neural Network)

### Определяне на награди

Наградата $r_t$ за всяка стъпка $t$ се дефинира като:

$$r = -1 + C_a$$

където $C_a$ е новият процент покритие на код след избора на действие $a$. "Наказанието", количествено оценено с $-1$ за всяко следващо взето действие "мотивира" агента да се стреми изучи средата максимално бързо. Това спомага за намаляване на възможността за разглеждане на две съседни състояния в цикъл.

### Оценка на модела за избор на действия

Адекватността на агента, т.е. адекватността на всички избрани от агента действия се измерва чрез т.нар мярка за "съжаление" (regret) - разликата между оптималната обща награда и получената обща награда. Оптималната награда може да се постигне, когато на всяка стъпка $t$, агентът избира оптимално действие $a^*$.

# Мотивация

Една от основните цели на Изкуствения Интелект (ИИ) е да създаде агенти, които разбират и взаимодействат със света около нас. Значителен прогрес в тази насока беше постигнат през последните години благодарение на развитието на изчислителната техника (графични ускорители), наличието на голямо количество данни, нови начини за събиране и съхранението им и нови алгоритми. Бързият напредък в сферата на подсиленото обучение доведе до разработката на интелигентни агенти, взаимодействащи с все по-сложни среди [@mnih2015human], [@silver2016alphago], [@levine2016end] и [@silver2017mastering]. Критични за това са обучаващите алгоритми, техники за скалирането им и симулационни среди, които предоставят начини за оценка и сравняване на различни агенти [@bellemare2013arcade], [@todorov2012mujoco] и [@johansson2016learning].

Хората се справят лесно с редица задачи, които изискват комплексно разбиране на визуалния свят, разпознаване на различни обекти в него и взаимодействие с тях. Например (екран от ГПИ среда и разпознаване на обекти в него). Би било лесно за човек да изучи подобна визуална среда.

Агенти, действащи в симулирани среди, са фундаментално ограничени - те никога не се сблъскват със сложността на реалния свят, поради което не могат да използват семантично знание и достигнат интелигентност. В роботиката агентите действат в реална среда, но процесът на обучение е бавен и скъп, дори и за тясно дефинирани задачи [@levine2016end].

За справянето с тези проблеми могат да се използват среди, базирани на ГПИ приложения [@pmlr-v70-shi17a]. Те предоставят разнообразни задачи, възможност за бързо итериране и обучение. Агентите получават същите сензорни данни, които получава човек, взаимодействащ с тези среди. Те предоставят възможност за изграждане на знание, невъзможно за придобиване в симулации.

**Предизвикателства** Тъй като подобни възможности изглеждат естествени за човек, може да забравим колко трудни са те за един агент. Изображенията са представени като голям масив от числа, които представят яркостта за всяка позиция. Едно такова изображение може да съдържа милиони такива *пиксели*, които агентът трябва да трансформира до семантични концепции на високо ниво, като например "текст" или "бутон". При това, различни форми и цветове на даден бутон, също трябва да се класифицират като такъв, независимо от възможността за наличие на напълно различни шаблони (patterns) в яркостта на пикселите.

Концептуалното разбиране на дадено изображение е само първата стъпка за създаването на подобни агенти. Основна задача е създаване на модел на агент, който избира действия, които до доведат да постигането на поставена задача. Трудността тук се изразява в липсата на пълна информация (fully observed) за средата в която агента действа. Например, натискането на един и същи бутон в различни състояния на средата, може да доведе до наблюдаването на две напълно различни състояния на средата. Това означава, че е необходимо знание за конкретното състояние на средата.

Агентът няма предварителен модел на средата, която изучава. Той я "опознава", чрез опити и грешки, като се опитва да приложи различни комбинации от действия в дадено състояние, за да постигне оптимална награда. На всяка стъпка даден агент трябва да избере дали да използва вече наученото или да избере действие, което не е изпълнил в конкретното състояние. Тази дилема се нарича: компромис на изучаване и използване на наученото (exploration exploitation tradeoff) и е основна задача за решаване от всеки агент. ГПИ средите могат да предоставят голям брой действия за дадено състояние (например меню системата на Microsoft Word), което прави пълното им изучаване неприложимо в кратки интервали от време.

**Обнадеждаващ прогрес** Въпреки сложността на задачата, през последните години се наблюдава значителен прогрес в областта на подсиленото обучение. По конкретно, развитието на Изкуствените Невронни Мрежи (ИНМ) (Artificial Neural Networks) и методи за създаване на Бейсови модели с милиони параметри доведоха до значително разширение на областите в които подсиленото обучение е приложимо. Алгоритми като Дълбоко Q-обучение (Deep Q-learning) допринасят за създаване на агенти, които надхвърлят възможностите на хората в тясно дефинирани задачи [@mnih2015human], както и по-широко приложими такива [@silver2017mastering]


**Неотговорени въпроси** Основният подход, използван в много от тези приложения е създаване на модел, който работи в добре дефинирани среди или такива в които се наблюдава пълна информация. Допълнително, агентите взимат решения на базата на изчислени точкови оценки. Възможно е това да намаля ефективността на тези модели, както и обяснението на взетите решения. Открит остава въпросът дали добавянето на вероятностно разбиране за средата може да се справи с тези проблеми [@bellemare2017distributional] [@anonymous2018efficient].

**Принос** В тази работа добавяме вероятностно разбиране за средата и разработваме модели и техники за ефективно Бейсово изучаване на ГПИ среди. Също така създаваме конкретна среда, която Агентът изучава. Например, агентът трябва да наблюдава дадено изображение и избере действие, което максимизира вероятността за постигане на поставена цел. Този модел ще опише процесът за взимане на решения използвайки вероятностни разпределения. С други думи, целта на работата е създаване на агент, който ефективно изучава визуални среди.

**Дългосрочна мотивация** Основен стремеж на работата е да направи принос към изграждането и развитието на мислещи машини, както и приложи създадените модели в конкретни приложения. Техниките предложени тук са стъпка към достигането на бъдеще, в което агентите могат ефективно да взаимодействат с реалния свят (или по-сложни виртуални среди) и изпълняват комплексни задачи.

**Краткосрочна мотивация** Създаване на автоматизирани тестове за оценка на качеството на софтуерен продукт използвайки агент. Търсене за семантични и логически грешки в дадена програмна ГПИ среда. Възпроизвеждане на стъпки, необходими за възпроизвеждане на грешка.

# Описание на работата на агента

## Дадено на агента

1. Изображение на ГПИ средата - пример ГПИ. примери + изображения
2. Изучи ГПИ средата - да достигне всички възможни състояние на средата
3. Възможни действия - 

Агентът щракване върху екрана (пример визуален)
Агентът натиска и задържа (пример визуален)
Агентът натиска и влачи курсора (пример визуален)
Агентът scroll-ва нагоре и надолу

![](./assets/mobile_app_structure.png)

## Задачи

- Да постигне 100% покритие на програмния код на даден софтуерен продукт (СП)
- Да генерира поредица от действия с които преминава през всички клонове на дървото, описващо възможните състояния на СП

# Модел

Предполагаме, че на стъпка $t$ получаваме изображение и множество от възможни действия, определящи текущото състояние на средата. Искаме да създадем модел, който при подадени така описаните входни данни ни дава апостериорно разпределение, описващо вероятностите всяко от възможните действия да ни доведе до състояние с оптимално увеличение на покритието на програмен код.

Предизвикателството в така поставената задача се състой във факта, че изображенията са сложни многомерни обекти, а оптималното действие във всяко състояние може да е различно от това в което и да е друго. Нашият модел трябва да изгради вътрешно представяне на средата (напр. да научи какво е бутон, граници на отделните елементи и кои действия да използва върху тях) и да научи оптималните действия за всяко състояние.

## Представяне на изображенията

Векторното представянето на изображения се базира на напредъка постигнат в компютърното зрение, където Конволюционните Невронни Мрежи (КНМ) са показали, че могат да превръщат сурови изображения в мощни представяния [cite], които позволяват създаването на модели, представящи се на човешко ниво в състезания като ImageNet classification challenge [cite]. КНМ може да се разглежда като функция  $CNN_{\theta_c}(I)$, която извлича характеристики за изображение $I$ и има параметри $\theta_c$

## Архитектура на модела

* Входен слой - броят на невроните е равен на броят на пикселите в изображението, което представя средата в текущото време (обикновено 6400)
* 1-ви скрит слой - 2-мерен конволюционен слой с 3 входни канала и 16 изходни. Размерността на ядрото (kernel) е 5.
* 2-ри скрит слой - нормализиращ слой за 16-те изходни канала от предходния слой.
... Повтаряме предходните 2 слоя още 2 пъти
- 7-ми скрит слой - напълно свързан (fully-connected) слой, който сплесква (flatten?) броя на измеренията до 1.
- 8-ми скрит слой - отпадащ (dropout) слой, който изключва *20%* от невроните в мрежата на всяка стъпка.
- Изходен слой - редуцира броя на невроните от предходния слой до броя на възможните действия на Агента

## Цел/Оптимизация/Тренировка/Обучение

Целта на създадения модел е да научи апостериорното разпределение на действията в дадено състояние на средата $s$ - формула. 

Определение 1 (функция на загубата на Хубер). Казваме, че 

Обучението ще се състой в минимизиране на т.нар функция на загубата на Хубер. Тя има свойствата на средна квадратична грешка, когато грешката е малка и тези на средна абсолютна грешка, когато грешката е голяма - това прави модела ни издръжлив (robust) на екстремни стойности (outliers) [cite].

### Памет

Ще запазваме преходите от състояние $s$ до $s'$ при избрано действие $a$ и получената награда $r$, които ще използваме за допълнително обучение на модела. Избирането на случайно подмножество от така запазените преходи

### Намиране на апостериорно вероятностно разпределение на действията

Политиката, която използват агентите в [cite] е епсилон-лакома с намаляща стойност. Тя се отличава с това, че избира случайни действия, за да събере данни в началната фаза на обучението по-късно намаля вероятността за избиране на случайно действие. Обученият агент избира действие с максимална награда. Вместо това, може да опитаме да минимизираме несигурността (uncertainty) на нашата НМ. Това се оказва сравнително лесно ползвайки Thompson Sampling.

#### Thompson Sampling

Thompson sampling е политика, която насърчава агента да изследва средата в която действа, като избира действие с максимална награда, използвайки текущото си познание за средата. В нашият случай, това може да направим като симулираме стохастичен преход през НМ и изберем действието с най-висока очаквана награда.

# Експерименти
